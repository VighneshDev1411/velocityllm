syntax = "proto3";

package inference;

option go_package = "github.com/VighneshDev1411/velocityllm/internal/grpc/pb";

// InferenceService defines the gRPC service for LLM inference
service InferenceService {
  // Inference performs synchronous inference
  rpc Inference(InferenceRequest) returns (InferenceResponse);
  
  // StreamInference performs streaming inference
  rpc StreamInference(InferenceRequest) returns (stream InferenceStreamResponse);
  
  // LoadModel loads a model into memory
  rpc LoadModel(LoadModelRequest) returns (LoadModelResponse);
  
  // UnloadModel unloads a model from memory
  rpc UnloadModel(UnloadModelRequest) returns (UnloadModelResponse);
  
  // GetModelInfo retrieves information about a loaded model
  rpc GetModelInfo(GetModelInfoRequest) returns (GetModelInfoResponse);
  
  // HealthCheck checks the health of the inference worker
  rpc HealthCheck(HealthCheckRequest) returns (HealthCheckResponse);
}

// InferenceRequest represents a request for LLM inference
message InferenceRequest {
  string request_id = 1;
  string model_name = 2;
  string prompt = 3;
  
  // Generation parameters
  int32 max_tokens = 4;
  float temperature = 5;
  float top_p = 6;
  float top_k = 7;
  repeated string stop_sequences = 8;
  
  // Streaming
  bool stream = 9;
  
  // Metadata
  map<string, string> metadata = 10;
  
  // Advanced options
  int32 batch_size = 11;
  bool use_cache = 12;
  int64 timeout_ms = 13;
}

// InferenceResponse represents a synchronous inference response
message InferenceResponse {
  string request_id = 1;
  string model_name = 2;
  string output = 3;
  
  // Token information
  int32 tokens_generated = 4;
  int32 tokens_prompt = 5;
  
  // Timing
  int64 inference_time_ms = 6;
  int64 queue_time_ms = 7;
  
  // Metadata
  map<string, string> metadata = 8;
  
  // Status
  bool success = 9;
  string error = 10;
}

// InferenceStreamResponse represents a streaming inference chunk
message InferenceStreamResponse {
  string request_id = 1;
  int32 chunk_index = 2;
  string token = 3;
  bool is_final = 4;
  
  // Final response metadata (only in last chunk)
  int32 total_tokens = 5;
  int64 total_time_ms = 6;
  map<string, string> metadata = 7;
}

// LoadModelRequest represents a request to load a model
message LoadModelRequest {
  string model_name = 1;
  string model_path = 2;
  
  // Model configuration
  string device = 3;  // "cuda", "cpu", "auto"
  int32 gpu_memory_mb = 4;
  string quantization = 5;  // "none", "int8", "int4", "fp16"
  int32 tensor_parallel_size = 6;
  
  // vLLM specific
  int32 max_model_len = 7;
  bool enable_prefix_caching = 8;
  
  // Metadata
  map<string, string> metadata = 9;
}

// LoadModelResponse represents the response to a load model request
message LoadModelResponse {
  bool success = 1;
  string error = 2;
  string model_name = 3;
  
  // Model information
  int64 model_size_mb = 4;
  int64 load_time_ms = 5;
  string device = 6;
  
  // Metadata
  map<string, string> metadata = 7;
}

// UnloadModelRequest represents a request to unload a model
message UnloadModelRequest {
  string model_name = 1;
  bool force = 2;  // Force unload even if in use
}

// UnloadModelResponse represents the response to an unload request
message UnloadModelResponse {
  bool success = 1;
  string error = 2;
  string model_name = 3;
  int64 unload_time_ms = 4;
}

// GetModelInfoRequest represents a request for model information
message GetModelInfoRequest {
  string model_name = 1;
}

// GetModelInfoResponse represents model information
message GetModelInfoResponse {
  bool success = 1;
  string error = 2;
  
  // Model details
  string model_name = 3;
  string model_path = 4;
  bool is_loaded = 5;
  
  // Resource usage
  int64 memory_usage_mb = 6;
  string device = 7;
  
  // Capabilities
  int32 max_context_length = 8;
  repeated string supported_features = 9;
  
  // Statistics
  int64 total_requests = 10;
  int64 total_tokens_generated = 11;
  float avg_inference_time_ms = 12;
  
  // Metadata
  map<string, string> metadata = 13;
}

// HealthCheckRequest represents a health check request
message HealthCheckRequest {
  bool detailed = 1;  // Return detailed health information
}

// HealthCheckResponse represents a health check response
message HealthCheckResponse {
  string status = 1;  // "healthy", "degraded", "unhealthy"
  
  // System resources
  float cpu_usage_percent = 2;
  float memory_usage_percent = 3;
  float gpu_usage_percent = 4;
  float gpu_memory_usage_percent = 5;
  
  // Worker status
  int32 active_requests = 6;
  int32 queued_requests = 7;
  int32 loaded_models = 8;
  
  // Timing
  int64 uptime_seconds = 9;
  int64 last_request_time_ms = 10;
  
  // Detailed info (if requested)
  repeated ModelHealth model_health = 11;
  
  // Metadata
  map<string, string> metadata = 12;
}

// ModelHealth represents health information for a specific model
message ModelHealth {
  string model_name = 1;
  bool is_healthy = 2;
  int64 memory_usage_mb = 3;
  int32 active_requests = 4;
  float avg_latency_ms = 5;
  string error = 6;
}

// BatchInferenceRequest represents a batch inference request
message BatchInferenceRequest {
  repeated InferenceRequest requests = 1;
  bool parallel = 2;  // Process requests in parallel
  int64 timeout_ms = 3;
}

// BatchInferenceResponse represents a batch inference response
message BatchInferenceResponse {
  repeated InferenceResponse responses = 1;
  int32 successful = 2;
  int32 failed = 3;
  int64 total_time_ms = 4;
}

// TokenizeRequest represents a request to tokenize text
message TokenizeRequest {
  string model_name = 1;
  string text = 2;
  bool add_special_tokens = 3;
}

// TokenizeResponse represents a tokenization response
message TokenizeResponse {
  bool success = 1;
  string error = 2;
  repeated int32 token_ids = 3;
  repeated string tokens = 4;
  int32 token_count = 5;
}

// DetokenizeRequest represents a request to detokenize token IDs
message DetokenizeRequest {
  string model_name = 1;
  repeated int32 token_ids = 2;
  bool skip_special_tokens = 3;
}

// DetokenizeResponse represents a detokenization response
message DetokenizeResponse {
  bool success = 1;
  string error = 2;
  string text = 3;
}

// WorkerStats represents worker statistics
message WorkerStats {
  string worker_id = 1;
  string status = 2;
  
  // Resource usage
  float cpu_usage = 3;
  float memory_usage_mb = 4;
  float gpu_usage = 5;
  float gpu_memory_mb = 6;
  
  // Request statistics
  int64 total_requests = 7;
  int64 successful_requests = 8;
  int64 failed_requests = 9;
  
  // Performance
  float avg_inference_time_ms = 10;
  float p50_latency_ms = 11;
  float p95_latency_ms = 12;
  float p99_latency_ms = 13;
  
  // Current state
  int32 active_requests = 14;
  int32 queued_requests = 15;
  
  // Uptime
  int64 uptime_seconds = 16;
  int64 last_request_timestamp = 17;
}

// ConfigUpdate represents a configuration update
message ConfigUpdate {
  string key = 1;
  string value = 2;
  bool apply_immediately = 3;
}

// ConfigUpdateResponse represents the response to a config update
message ConfigUpdateResponse {
  bool success = 1;
  string error = 2;
  string previous_value = 3;
  string new_value = 4;
}
