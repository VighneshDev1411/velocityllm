// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v5.29.3
// source: proto/inference.proto

package pb

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// InferenceRequest represents a request for LLM inference
type InferenceRequest struct {
	state     protoimpl.MessageState `protogen:"open.v1"`
	RequestId string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	ModelName string                 `protobuf:"bytes,2,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	Prompt    string                 `protobuf:"bytes,3,opt,name=prompt,proto3" json:"prompt,omitempty"`
	// Generation parameters
	MaxTokens     int32    `protobuf:"varint,4,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	Temperature   float32  `protobuf:"fixed32,5,opt,name=temperature,proto3" json:"temperature,omitempty"`
	TopP          float32  `protobuf:"fixed32,6,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`
	TopK          float32  `protobuf:"fixed32,7,opt,name=top_k,json=topK,proto3" json:"top_k,omitempty"`
	StopSequences []string `protobuf:"bytes,8,rep,name=stop_sequences,json=stopSequences,proto3" json:"stop_sequences,omitempty"`
	// Streaming
	Stream bool `protobuf:"varint,9,opt,name=stream,proto3" json:"stream,omitempty"`
	// Metadata
	Metadata map[string]string `protobuf:"bytes,10,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Advanced options
	BatchSize     int32 `protobuf:"varint,11,opt,name=batch_size,json=batchSize,proto3" json:"batch_size,omitempty"`
	UseCache      bool  `protobuf:"varint,12,opt,name=use_cache,json=useCache,proto3" json:"use_cache,omitempty"`
	TimeoutMs     int64 `protobuf:"varint,13,opt,name=timeout_ms,json=timeoutMs,proto3" json:"timeout_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferenceRequest) Reset() {
	*x = InferenceRequest{}
	mi := &file_proto_inference_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceRequest) ProtoMessage() {}

func (x *InferenceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceRequest.ProtoReflect.Descriptor instead.
func (*InferenceRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{0}
}

func (x *InferenceRequest) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *InferenceRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *InferenceRequest) GetPrompt() string {
	if x != nil {
		return x.Prompt
	}
	return ""
}

func (x *InferenceRequest) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *InferenceRequest) GetTemperature() float32 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *InferenceRequest) GetTopP() float32 {
	if x != nil {
		return x.TopP
	}
	return 0
}

func (x *InferenceRequest) GetTopK() float32 {
	if x != nil {
		return x.TopK
	}
	return 0
}

func (x *InferenceRequest) GetStopSequences() []string {
	if x != nil {
		return x.StopSequences
	}
	return nil
}

func (x *InferenceRequest) GetStream() bool {
	if x != nil {
		return x.Stream
	}
	return false
}

func (x *InferenceRequest) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

func (x *InferenceRequest) GetBatchSize() int32 {
	if x != nil {
		return x.BatchSize
	}
	return 0
}

func (x *InferenceRequest) GetUseCache() bool {
	if x != nil {
		return x.UseCache
	}
	return false
}

func (x *InferenceRequest) GetTimeoutMs() int64 {
	if x != nil {
		return x.TimeoutMs
	}
	return 0
}

// InferenceResponse represents a synchronous inference response
type InferenceResponse struct {
	state     protoimpl.MessageState `protogen:"open.v1"`
	RequestId string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	ModelName string                 `protobuf:"bytes,2,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	Output    string                 `protobuf:"bytes,3,opt,name=output,proto3" json:"output,omitempty"`
	// Token information
	TokensGenerated int32 `protobuf:"varint,4,opt,name=tokens_generated,json=tokensGenerated,proto3" json:"tokens_generated,omitempty"`
	TokensPrompt    int32 `protobuf:"varint,5,opt,name=tokens_prompt,json=tokensPrompt,proto3" json:"tokens_prompt,omitempty"`
	// Timing
	InferenceTimeMs int64 `protobuf:"varint,6,opt,name=inference_time_ms,json=inferenceTimeMs,proto3" json:"inference_time_ms,omitempty"`
	QueueTimeMs     int64 `protobuf:"varint,7,opt,name=queue_time_ms,json=queueTimeMs,proto3" json:"queue_time_ms,omitempty"`
	// Metadata
	Metadata map[string]string `protobuf:"bytes,8,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Status
	Success       bool   `protobuf:"varint,9,opt,name=success,proto3" json:"success,omitempty"`
	Error         string `protobuf:"bytes,10,opt,name=error,proto3" json:"error,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferenceResponse) Reset() {
	*x = InferenceResponse{}
	mi := &file_proto_inference_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceResponse) ProtoMessage() {}

func (x *InferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceResponse.ProtoReflect.Descriptor instead.
func (*InferenceResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{1}
}

func (x *InferenceResponse) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *InferenceResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *InferenceResponse) GetOutput() string {
	if x != nil {
		return x.Output
	}
	return ""
}

func (x *InferenceResponse) GetTokensGenerated() int32 {
	if x != nil {
		return x.TokensGenerated
	}
	return 0
}

func (x *InferenceResponse) GetTokensPrompt() int32 {
	if x != nil {
		return x.TokensPrompt
	}
	return 0
}

func (x *InferenceResponse) GetInferenceTimeMs() int64 {
	if x != nil {
		return x.InferenceTimeMs
	}
	return 0
}

func (x *InferenceResponse) GetQueueTimeMs() int64 {
	if x != nil {
		return x.QueueTimeMs
	}
	return 0
}

func (x *InferenceResponse) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

func (x *InferenceResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *InferenceResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

// InferenceStreamResponse represents a streaming inference chunk
type InferenceStreamResponse struct {
	state      protoimpl.MessageState `protogen:"open.v1"`
	RequestId  string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	ChunkIndex int32                  `protobuf:"varint,2,opt,name=chunk_index,json=chunkIndex,proto3" json:"chunk_index,omitempty"`
	Token      string                 `protobuf:"bytes,3,opt,name=token,proto3" json:"token,omitempty"`
	IsFinal    bool                   `protobuf:"varint,4,opt,name=is_final,json=isFinal,proto3" json:"is_final,omitempty"`
	// Final response metadata (only in last chunk)
	TotalTokens   int32             `protobuf:"varint,5,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	TotalTimeMs   int64             `protobuf:"varint,6,opt,name=total_time_ms,json=totalTimeMs,proto3" json:"total_time_ms,omitempty"`
	Metadata      map[string]string `protobuf:"bytes,7,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferenceStreamResponse) Reset() {
	*x = InferenceStreamResponse{}
	mi := &file_proto_inference_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferenceStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferenceStreamResponse) ProtoMessage() {}

func (x *InferenceStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferenceStreamResponse.ProtoReflect.Descriptor instead.
func (*InferenceStreamResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{2}
}

func (x *InferenceStreamResponse) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *InferenceStreamResponse) GetChunkIndex() int32 {
	if x != nil {
		return x.ChunkIndex
	}
	return 0
}

func (x *InferenceStreamResponse) GetToken() string {
	if x != nil {
		return x.Token
	}
	return ""
}

func (x *InferenceStreamResponse) GetIsFinal() bool {
	if x != nil {
		return x.IsFinal
	}
	return false
}

func (x *InferenceStreamResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *InferenceStreamResponse) GetTotalTimeMs() int64 {
	if x != nil {
		return x.TotalTimeMs
	}
	return 0
}

func (x *InferenceStreamResponse) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// LoadModelRequest represents a request to load a model
type LoadModelRequest struct {
	state     protoimpl.MessageState `protogen:"open.v1"`
	ModelName string                 `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	ModelPath string                 `protobuf:"bytes,2,opt,name=model_path,json=modelPath,proto3" json:"model_path,omitempty"`
	// Model configuration
	Device             string `protobuf:"bytes,3,opt,name=device,proto3" json:"device,omitempty"` // "cuda", "cpu", "auto"
	GpuMemoryMb        int32  `protobuf:"varint,4,opt,name=gpu_memory_mb,json=gpuMemoryMb,proto3" json:"gpu_memory_mb,omitempty"`
	Quantization       string `protobuf:"bytes,5,opt,name=quantization,proto3" json:"quantization,omitempty"` // "none", "int8", "int4", "fp16"
	TensorParallelSize int32  `protobuf:"varint,6,opt,name=tensor_parallel_size,json=tensorParallelSize,proto3" json:"tensor_parallel_size,omitempty"`
	// vLLM specific
	MaxModelLen         int32 `protobuf:"varint,7,opt,name=max_model_len,json=maxModelLen,proto3" json:"max_model_len,omitempty"`
	EnablePrefixCaching bool  `protobuf:"varint,8,opt,name=enable_prefix_caching,json=enablePrefixCaching,proto3" json:"enable_prefix_caching,omitempty"`
	// Metadata
	Metadata      map[string]string `protobuf:"bytes,9,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LoadModelRequest) Reset() {
	*x = LoadModelRequest{}
	mi := &file_proto_inference_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LoadModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LoadModelRequest) ProtoMessage() {}

func (x *LoadModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LoadModelRequest.ProtoReflect.Descriptor instead.
func (*LoadModelRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{3}
}

func (x *LoadModelRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *LoadModelRequest) GetModelPath() string {
	if x != nil {
		return x.ModelPath
	}
	return ""
}

func (x *LoadModelRequest) GetDevice() string {
	if x != nil {
		return x.Device
	}
	return ""
}

func (x *LoadModelRequest) GetGpuMemoryMb() int32 {
	if x != nil {
		return x.GpuMemoryMb
	}
	return 0
}

func (x *LoadModelRequest) GetQuantization() string {
	if x != nil {
		return x.Quantization
	}
	return ""
}

func (x *LoadModelRequest) GetTensorParallelSize() int32 {
	if x != nil {
		return x.TensorParallelSize
	}
	return 0
}

func (x *LoadModelRequest) GetMaxModelLen() int32 {
	if x != nil {
		return x.MaxModelLen
	}
	return 0
}

func (x *LoadModelRequest) GetEnablePrefixCaching() bool {
	if x != nil {
		return x.EnablePrefixCaching
	}
	return false
}

func (x *LoadModelRequest) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// LoadModelResponse represents the response to a load model request
type LoadModelResponse struct {
	state     protoimpl.MessageState `protogen:"open.v1"`
	Success   bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error     string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	ModelName string                 `protobuf:"bytes,3,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// Model information
	ModelSizeMb int64  `protobuf:"varint,4,opt,name=model_size_mb,json=modelSizeMb,proto3" json:"model_size_mb,omitempty"`
	LoadTimeMs  int64  `protobuf:"varint,5,opt,name=load_time_ms,json=loadTimeMs,proto3" json:"load_time_ms,omitempty"`
	Device      string `protobuf:"bytes,6,opt,name=device,proto3" json:"device,omitempty"`
	// Metadata
	Metadata      map[string]string `protobuf:"bytes,7,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LoadModelResponse) Reset() {
	*x = LoadModelResponse{}
	mi := &file_proto_inference_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LoadModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LoadModelResponse) ProtoMessage() {}

func (x *LoadModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LoadModelResponse.ProtoReflect.Descriptor instead.
func (*LoadModelResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{4}
}

func (x *LoadModelResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *LoadModelResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *LoadModelResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *LoadModelResponse) GetModelSizeMb() int64 {
	if x != nil {
		return x.ModelSizeMb
	}
	return 0
}

func (x *LoadModelResponse) GetLoadTimeMs() int64 {
	if x != nil {
		return x.LoadTimeMs
	}
	return 0
}

func (x *LoadModelResponse) GetDevice() string {
	if x != nil {
		return x.Device
	}
	return ""
}

func (x *LoadModelResponse) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// UnloadModelRequest represents a request to unload a model
type UnloadModelRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelName     string                 `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	Force         bool                   `protobuf:"varint,2,opt,name=force,proto3" json:"force,omitempty"` // Force unload even if in use
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UnloadModelRequest) Reset() {
	*x = UnloadModelRequest{}
	mi := &file_proto_inference_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UnloadModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UnloadModelRequest) ProtoMessage() {}

func (x *UnloadModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UnloadModelRequest.ProtoReflect.Descriptor instead.
func (*UnloadModelRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{5}
}

func (x *UnloadModelRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *UnloadModelRequest) GetForce() bool {
	if x != nil {
		return x.Force
	}
	return false
}

// UnloadModelResponse represents the response to an unload request
type UnloadModelResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Success       bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error         string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	ModelName     string                 `protobuf:"bytes,3,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	UnloadTimeMs  int64                  `protobuf:"varint,4,opt,name=unload_time_ms,json=unloadTimeMs,proto3" json:"unload_time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UnloadModelResponse) Reset() {
	*x = UnloadModelResponse{}
	mi := &file_proto_inference_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UnloadModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UnloadModelResponse) ProtoMessage() {}

func (x *UnloadModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UnloadModelResponse.ProtoReflect.Descriptor instead.
func (*UnloadModelResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{6}
}

func (x *UnloadModelResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *UnloadModelResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *UnloadModelResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *UnloadModelResponse) GetUnloadTimeMs() int64 {
	if x != nil {
		return x.UnloadTimeMs
	}
	return 0
}

// GetModelInfoRequest represents a request for model information
type GetModelInfoRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelName     string                 `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetModelInfoRequest) Reset() {
	*x = GetModelInfoRequest{}
	mi := &file_proto_inference_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelInfoRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelInfoRequest) ProtoMessage() {}

func (x *GetModelInfoRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetModelInfoRequest.ProtoReflect.Descriptor instead.
func (*GetModelInfoRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{7}
}

func (x *GetModelInfoRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

// GetModelInfoResponse represents model information
type GetModelInfoResponse struct {
	state   protoimpl.MessageState `protogen:"open.v1"`
	Success bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error   string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	// Model details
	ModelName string `protobuf:"bytes,3,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	ModelPath string `protobuf:"bytes,4,opt,name=model_path,json=modelPath,proto3" json:"model_path,omitempty"`
	IsLoaded  bool   `protobuf:"varint,5,opt,name=is_loaded,json=isLoaded,proto3" json:"is_loaded,omitempty"`
	// Resource usage
	MemoryUsageMb int64  `protobuf:"varint,6,opt,name=memory_usage_mb,json=memoryUsageMb,proto3" json:"memory_usage_mb,omitempty"`
	Device        string `protobuf:"bytes,7,opt,name=device,proto3" json:"device,omitempty"`
	// Capabilities
	MaxContextLength  int32    `protobuf:"varint,8,opt,name=max_context_length,json=maxContextLength,proto3" json:"max_context_length,omitempty"`
	SupportedFeatures []string `protobuf:"bytes,9,rep,name=supported_features,json=supportedFeatures,proto3" json:"supported_features,omitempty"`
	// Statistics
	TotalRequests        int64   `protobuf:"varint,10,opt,name=total_requests,json=totalRequests,proto3" json:"total_requests,omitempty"`
	TotalTokensGenerated int64   `protobuf:"varint,11,opt,name=total_tokens_generated,json=totalTokensGenerated,proto3" json:"total_tokens_generated,omitempty"`
	AvgInferenceTimeMs   float32 `protobuf:"fixed32,12,opt,name=avg_inference_time_ms,json=avgInferenceTimeMs,proto3" json:"avg_inference_time_ms,omitempty"`
	// Metadata
	Metadata      map[string]string `protobuf:"bytes,13,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetModelInfoResponse) Reset() {
	*x = GetModelInfoResponse{}
	mi := &file_proto_inference_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelInfoResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelInfoResponse) ProtoMessage() {}

func (x *GetModelInfoResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetModelInfoResponse.ProtoReflect.Descriptor instead.
func (*GetModelInfoResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{8}
}

func (x *GetModelInfoResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *GetModelInfoResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *GetModelInfoResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *GetModelInfoResponse) GetModelPath() string {
	if x != nil {
		return x.ModelPath
	}
	return ""
}

func (x *GetModelInfoResponse) GetIsLoaded() bool {
	if x != nil {
		return x.IsLoaded
	}
	return false
}

func (x *GetModelInfoResponse) GetMemoryUsageMb() int64 {
	if x != nil {
		return x.MemoryUsageMb
	}
	return 0
}

func (x *GetModelInfoResponse) GetDevice() string {
	if x != nil {
		return x.Device
	}
	return ""
}

func (x *GetModelInfoResponse) GetMaxContextLength() int32 {
	if x != nil {
		return x.MaxContextLength
	}
	return 0
}

func (x *GetModelInfoResponse) GetSupportedFeatures() []string {
	if x != nil {
		return x.SupportedFeatures
	}
	return nil
}

func (x *GetModelInfoResponse) GetTotalRequests() int64 {
	if x != nil {
		return x.TotalRequests
	}
	return 0
}

func (x *GetModelInfoResponse) GetTotalTokensGenerated() int64 {
	if x != nil {
		return x.TotalTokensGenerated
	}
	return 0
}

func (x *GetModelInfoResponse) GetAvgInferenceTimeMs() float32 {
	if x != nil {
		return x.AvgInferenceTimeMs
	}
	return 0
}

func (x *GetModelInfoResponse) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// HealthCheckRequest represents a health check request
type HealthCheckRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Detailed      bool                   `protobuf:"varint,1,opt,name=detailed,proto3" json:"detailed,omitempty"` // Return detailed health information
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *HealthCheckRequest) Reset() {
	*x = HealthCheckRequest{}
	mi := &file_proto_inference_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *HealthCheckRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*HealthCheckRequest) ProtoMessage() {}

func (x *HealthCheckRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use HealthCheckRequest.ProtoReflect.Descriptor instead.
func (*HealthCheckRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{9}
}

func (x *HealthCheckRequest) GetDetailed() bool {
	if x != nil {
		return x.Detailed
	}
	return false
}

// HealthCheckResponse represents a health check response
type HealthCheckResponse struct {
	state  protoimpl.MessageState `protogen:"open.v1"`
	Status string                 `protobuf:"bytes,1,opt,name=status,proto3" json:"status,omitempty"` // "healthy", "degraded", "unhealthy"
	// System resources
	CpuUsagePercent       float32 `protobuf:"fixed32,2,opt,name=cpu_usage_percent,json=cpuUsagePercent,proto3" json:"cpu_usage_percent,omitempty"`
	MemoryUsagePercent    float32 `protobuf:"fixed32,3,opt,name=memory_usage_percent,json=memoryUsagePercent,proto3" json:"memory_usage_percent,omitempty"`
	GpuUsagePercent       float32 `protobuf:"fixed32,4,opt,name=gpu_usage_percent,json=gpuUsagePercent,proto3" json:"gpu_usage_percent,omitempty"`
	GpuMemoryUsagePercent float32 `protobuf:"fixed32,5,opt,name=gpu_memory_usage_percent,json=gpuMemoryUsagePercent,proto3" json:"gpu_memory_usage_percent,omitempty"`
	// Worker status
	ActiveRequests int32 `protobuf:"varint,6,opt,name=active_requests,json=activeRequests,proto3" json:"active_requests,omitempty"`
	QueuedRequests int32 `protobuf:"varint,7,opt,name=queued_requests,json=queuedRequests,proto3" json:"queued_requests,omitempty"`
	LoadedModels   int32 `protobuf:"varint,8,opt,name=loaded_models,json=loadedModels,proto3" json:"loaded_models,omitempty"`
	// Timing
	UptimeSeconds     int64 `protobuf:"varint,9,opt,name=uptime_seconds,json=uptimeSeconds,proto3" json:"uptime_seconds,omitempty"`
	LastRequestTimeMs int64 `protobuf:"varint,10,opt,name=last_request_time_ms,json=lastRequestTimeMs,proto3" json:"last_request_time_ms,omitempty"`
	// Detailed info (if requested)
	ModelHealth []*ModelHealth `protobuf:"bytes,11,rep,name=model_health,json=modelHealth,proto3" json:"model_health,omitempty"`
	// Metadata
	Metadata      map[string]string `protobuf:"bytes,12,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *HealthCheckResponse) Reset() {
	*x = HealthCheckResponse{}
	mi := &file_proto_inference_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *HealthCheckResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*HealthCheckResponse) ProtoMessage() {}

func (x *HealthCheckResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use HealthCheckResponse.ProtoReflect.Descriptor instead.
func (*HealthCheckResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{10}
}

func (x *HealthCheckResponse) GetStatus() string {
	if x != nil {
		return x.Status
	}
	return ""
}

func (x *HealthCheckResponse) GetCpuUsagePercent() float32 {
	if x != nil {
		return x.CpuUsagePercent
	}
	return 0
}

func (x *HealthCheckResponse) GetMemoryUsagePercent() float32 {
	if x != nil {
		return x.MemoryUsagePercent
	}
	return 0
}

func (x *HealthCheckResponse) GetGpuUsagePercent() float32 {
	if x != nil {
		return x.GpuUsagePercent
	}
	return 0
}

func (x *HealthCheckResponse) GetGpuMemoryUsagePercent() float32 {
	if x != nil {
		return x.GpuMemoryUsagePercent
	}
	return 0
}

func (x *HealthCheckResponse) GetActiveRequests() int32 {
	if x != nil {
		return x.ActiveRequests
	}
	return 0
}

func (x *HealthCheckResponse) GetQueuedRequests() int32 {
	if x != nil {
		return x.QueuedRequests
	}
	return 0
}

func (x *HealthCheckResponse) GetLoadedModels() int32 {
	if x != nil {
		return x.LoadedModels
	}
	return 0
}

func (x *HealthCheckResponse) GetUptimeSeconds() int64 {
	if x != nil {
		return x.UptimeSeconds
	}
	return 0
}

func (x *HealthCheckResponse) GetLastRequestTimeMs() int64 {
	if x != nil {
		return x.LastRequestTimeMs
	}
	return 0
}

func (x *HealthCheckResponse) GetModelHealth() []*ModelHealth {
	if x != nil {
		return x.ModelHealth
	}
	return nil
}

func (x *HealthCheckResponse) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// ModelHealth represents health information for a specific model
type ModelHealth struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	ModelName      string                 `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	IsHealthy      bool                   `protobuf:"varint,2,opt,name=is_healthy,json=isHealthy,proto3" json:"is_healthy,omitempty"`
	MemoryUsageMb  int64                  `protobuf:"varint,3,opt,name=memory_usage_mb,json=memoryUsageMb,proto3" json:"memory_usage_mb,omitempty"`
	ActiveRequests int32                  `protobuf:"varint,4,opt,name=active_requests,json=activeRequests,proto3" json:"active_requests,omitempty"`
	AvgLatencyMs   float32                `protobuf:"fixed32,5,opt,name=avg_latency_ms,json=avgLatencyMs,proto3" json:"avg_latency_ms,omitempty"`
	Error          string                 `protobuf:"bytes,6,opt,name=error,proto3" json:"error,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *ModelHealth) Reset() {
	*x = ModelHealth{}
	mi := &file_proto_inference_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelHealth) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelHealth) ProtoMessage() {}

func (x *ModelHealth) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelHealth.ProtoReflect.Descriptor instead.
func (*ModelHealth) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{11}
}

func (x *ModelHealth) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *ModelHealth) GetIsHealthy() bool {
	if x != nil {
		return x.IsHealthy
	}
	return false
}

func (x *ModelHealth) GetMemoryUsageMb() int64 {
	if x != nil {
		return x.MemoryUsageMb
	}
	return 0
}

func (x *ModelHealth) GetActiveRequests() int32 {
	if x != nil {
		return x.ActiveRequests
	}
	return 0
}

func (x *ModelHealth) GetAvgLatencyMs() float32 {
	if x != nil {
		return x.AvgLatencyMs
	}
	return 0
}

func (x *ModelHealth) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

// BatchInferenceRequest represents a batch inference request
type BatchInferenceRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Requests      []*InferenceRequest    `protobuf:"bytes,1,rep,name=requests,proto3" json:"requests,omitempty"`
	Parallel      bool                   `protobuf:"varint,2,opt,name=parallel,proto3" json:"parallel,omitempty"` // Process requests in parallel
	TimeoutMs     int64                  `protobuf:"varint,3,opt,name=timeout_ms,json=timeoutMs,proto3" json:"timeout_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchInferenceRequest) Reset() {
	*x = BatchInferenceRequest{}
	mi := &file_proto_inference_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceRequest) ProtoMessage() {}

func (x *BatchInferenceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceRequest.ProtoReflect.Descriptor instead.
func (*BatchInferenceRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{12}
}

func (x *BatchInferenceRequest) GetRequests() []*InferenceRequest {
	if x != nil {
		return x.Requests
	}
	return nil
}

func (x *BatchInferenceRequest) GetParallel() bool {
	if x != nil {
		return x.Parallel
	}
	return false
}

func (x *BatchInferenceRequest) GetTimeoutMs() int64 {
	if x != nil {
		return x.TimeoutMs
	}
	return 0
}

// BatchInferenceResponse represents a batch inference response
type BatchInferenceResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Responses     []*InferenceResponse   `protobuf:"bytes,1,rep,name=responses,proto3" json:"responses,omitempty"`
	Successful    int32                  `protobuf:"varint,2,opt,name=successful,proto3" json:"successful,omitempty"`
	Failed        int32                  `protobuf:"varint,3,opt,name=failed,proto3" json:"failed,omitempty"`
	TotalTimeMs   int64                  `protobuf:"varint,4,opt,name=total_time_ms,json=totalTimeMs,proto3" json:"total_time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchInferenceResponse) Reset() {
	*x = BatchInferenceResponse{}
	mi := &file_proto_inference_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceResponse) ProtoMessage() {}

func (x *BatchInferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceResponse.ProtoReflect.Descriptor instead.
func (*BatchInferenceResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{13}
}

func (x *BatchInferenceResponse) GetResponses() []*InferenceResponse {
	if x != nil {
		return x.Responses
	}
	return nil
}

func (x *BatchInferenceResponse) GetSuccessful() int32 {
	if x != nil {
		return x.Successful
	}
	return 0
}

func (x *BatchInferenceResponse) GetFailed() int32 {
	if x != nil {
		return x.Failed
	}
	return 0
}

func (x *BatchInferenceResponse) GetTotalTimeMs() int64 {
	if x != nil {
		return x.TotalTimeMs
	}
	return 0
}

// TokenizeRequest represents a request to tokenize text
type TokenizeRequest struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	ModelName        string                 `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	Text             string                 `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	AddSpecialTokens bool                   `protobuf:"varint,3,opt,name=add_special_tokens,json=addSpecialTokens,proto3" json:"add_special_tokens,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *TokenizeRequest) Reset() {
	*x = TokenizeRequest{}
	mi := &file_proto_inference_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TokenizeRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TokenizeRequest) ProtoMessage() {}

func (x *TokenizeRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TokenizeRequest.ProtoReflect.Descriptor instead.
func (*TokenizeRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{14}
}

func (x *TokenizeRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *TokenizeRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *TokenizeRequest) GetAddSpecialTokens() bool {
	if x != nil {
		return x.AddSpecialTokens
	}
	return false
}

// TokenizeResponse represents a tokenization response
type TokenizeResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Success       bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error         string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	TokenIds      []int32                `protobuf:"varint,3,rep,packed,name=token_ids,json=tokenIds,proto3" json:"token_ids,omitempty"`
	Tokens        []string               `protobuf:"bytes,4,rep,name=tokens,proto3" json:"tokens,omitempty"`
	TokenCount    int32                  `protobuf:"varint,5,opt,name=token_count,json=tokenCount,proto3" json:"token_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TokenizeResponse) Reset() {
	*x = TokenizeResponse{}
	mi := &file_proto_inference_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TokenizeResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TokenizeResponse) ProtoMessage() {}

func (x *TokenizeResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TokenizeResponse.ProtoReflect.Descriptor instead.
func (*TokenizeResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{15}
}

func (x *TokenizeResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *TokenizeResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *TokenizeResponse) GetTokenIds() []int32 {
	if x != nil {
		return x.TokenIds
	}
	return nil
}

func (x *TokenizeResponse) GetTokens() []string {
	if x != nil {
		return x.Tokens
	}
	return nil
}

func (x *TokenizeResponse) GetTokenCount() int32 {
	if x != nil {
		return x.TokenCount
	}
	return 0
}

// DetokenizeRequest represents a request to detokenize token IDs
type DetokenizeRequest struct {
	state             protoimpl.MessageState `protogen:"open.v1"`
	ModelName         string                 `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	TokenIds          []int32                `protobuf:"varint,2,rep,packed,name=token_ids,json=tokenIds,proto3" json:"token_ids,omitempty"`
	SkipSpecialTokens bool                   `protobuf:"varint,3,opt,name=skip_special_tokens,json=skipSpecialTokens,proto3" json:"skip_special_tokens,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *DetokenizeRequest) Reset() {
	*x = DetokenizeRequest{}
	mi := &file_proto_inference_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DetokenizeRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DetokenizeRequest) ProtoMessage() {}

func (x *DetokenizeRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DetokenizeRequest.ProtoReflect.Descriptor instead.
func (*DetokenizeRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{16}
}

func (x *DetokenizeRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *DetokenizeRequest) GetTokenIds() []int32 {
	if x != nil {
		return x.TokenIds
	}
	return nil
}

func (x *DetokenizeRequest) GetSkipSpecialTokens() bool {
	if x != nil {
		return x.SkipSpecialTokens
	}
	return false
}

// DetokenizeResponse represents a detokenization response
type DetokenizeResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Success       bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error         string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	Text          string                 `protobuf:"bytes,3,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DetokenizeResponse) Reset() {
	*x = DetokenizeResponse{}
	mi := &file_proto_inference_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DetokenizeResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DetokenizeResponse) ProtoMessage() {}

func (x *DetokenizeResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DetokenizeResponse.ProtoReflect.Descriptor instead.
func (*DetokenizeResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{17}
}

func (x *DetokenizeResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *DetokenizeResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *DetokenizeResponse) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

// WorkerStats represents worker statistics
type WorkerStats struct {
	state    protoimpl.MessageState `protogen:"open.v1"`
	WorkerId string                 `protobuf:"bytes,1,opt,name=worker_id,json=workerId,proto3" json:"worker_id,omitempty"`
	Status   string                 `protobuf:"bytes,2,opt,name=status,proto3" json:"status,omitempty"`
	// Resource usage
	CpuUsage      float32 `protobuf:"fixed32,3,opt,name=cpu_usage,json=cpuUsage,proto3" json:"cpu_usage,omitempty"`
	MemoryUsageMb float32 `protobuf:"fixed32,4,opt,name=memory_usage_mb,json=memoryUsageMb,proto3" json:"memory_usage_mb,omitempty"`
	GpuUsage      float32 `protobuf:"fixed32,5,opt,name=gpu_usage,json=gpuUsage,proto3" json:"gpu_usage,omitempty"`
	GpuMemoryMb   float32 `protobuf:"fixed32,6,opt,name=gpu_memory_mb,json=gpuMemoryMb,proto3" json:"gpu_memory_mb,omitempty"`
	// Request statistics
	TotalRequests      int64 `protobuf:"varint,7,opt,name=total_requests,json=totalRequests,proto3" json:"total_requests,omitempty"`
	SuccessfulRequests int64 `protobuf:"varint,8,opt,name=successful_requests,json=successfulRequests,proto3" json:"successful_requests,omitempty"`
	FailedRequests     int64 `protobuf:"varint,9,opt,name=failed_requests,json=failedRequests,proto3" json:"failed_requests,omitempty"`
	// Performance
	AvgInferenceTimeMs float32 `protobuf:"fixed32,10,opt,name=avg_inference_time_ms,json=avgInferenceTimeMs,proto3" json:"avg_inference_time_ms,omitempty"`
	P50LatencyMs       float32 `protobuf:"fixed32,11,opt,name=p50_latency_ms,json=p50LatencyMs,proto3" json:"p50_latency_ms,omitempty"`
	P95LatencyMs       float32 `protobuf:"fixed32,12,opt,name=p95_latency_ms,json=p95LatencyMs,proto3" json:"p95_latency_ms,omitempty"`
	P99LatencyMs       float32 `protobuf:"fixed32,13,opt,name=p99_latency_ms,json=p99LatencyMs,proto3" json:"p99_latency_ms,omitempty"`
	// Current state
	ActiveRequests int32 `protobuf:"varint,14,opt,name=active_requests,json=activeRequests,proto3" json:"active_requests,omitempty"`
	QueuedRequests int32 `protobuf:"varint,15,opt,name=queued_requests,json=queuedRequests,proto3" json:"queued_requests,omitempty"`
	// Uptime
	UptimeSeconds        int64 `protobuf:"varint,16,opt,name=uptime_seconds,json=uptimeSeconds,proto3" json:"uptime_seconds,omitempty"`
	LastRequestTimestamp int64 `protobuf:"varint,17,opt,name=last_request_timestamp,json=lastRequestTimestamp,proto3" json:"last_request_timestamp,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *WorkerStats) Reset() {
	*x = WorkerStats{}
	mi := &file_proto_inference_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *WorkerStats) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*WorkerStats) ProtoMessage() {}

func (x *WorkerStats) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use WorkerStats.ProtoReflect.Descriptor instead.
func (*WorkerStats) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{18}
}

func (x *WorkerStats) GetWorkerId() string {
	if x != nil {
		return x.WorkerId
	}
	return ""
}

func (x *WorkerStats) GetStatus() string {
	if x != nil {
		return x.Status
	}
	return ""
}

func (x *WorkerStats) GetCpuUsage() float32 {
	if x != nil {
		return x.CpuUsage
	}
	return 0
}

func (x *WorkerStats) GetMemoryUsageMb() float32 {
	if x != nil {
		return x.MemoryUsageMb
	}
	return 0
}

func (x *WorkerStats) GetGpuUsage() float32 {
	if x != nil {
		return x.GpuUsage
	}
	return 0
}

func (x *WorkerStats) GetGpuMemoryMb() float32 {
	if x != nil {
		return x.GpuMemoryMb
	}
	return 0
}

func (x *WorkerStats) GetTotalRequests() int64 {
	if x != nil {
		return x.TotalRequests
	}
	return 0
}

func (x *WorkerStats) GetSuccessfulRequests() int64 {
	if x != nil {
		return x.SuccessfulRequests
	}
	return 0
}

func (x *WorkerStats) GetFailedRequests() int64 {
	if x != nil {
		return x.FailedRequests
	}
	return 0
}

func (x *WorkerStats) GetAvgInferenceTimeMs() float32 {
	if x != nil {
		return x.AvgInferenceTimeMs
	}
	return 0
}

func (x *WorkerStats) GetP50LatencyMs() float32 {
	if x != nil {
		return x.P50LatencyMs
	}
	return 0
}

func (x *WorkerStats) GetP95LatencyMs() float32 {
	if x != nil {
		return x.P95LatencyMs
	}
	return 0
}

func (x *WorkerStats) GetP99LatencyMs() float32 {
	if x != nil {
		return x.P99LatencyMs
	}
	return 0
}

func (x *WorkerStats) GetActiveRequests() int32 {
	if x != nil {
		return x.ActiveRequests
	}
	return 0
}

func (x *WorkerStats) GetQueuedRequests() int32 {
	if x != nil {
		return x.QueuedRequests
	}
	return 0
}

func (x *WorkerStats) GetUptimeSeconds() int64 {
	if x != nil {
		return x.UptimeSeconds
	}
	return 0
}

func (x *WorkerStats) GetLastRequestTimestamp() int64 {
	if x != nil {
		return x.LastRequestTimestamp
	}
	return 0
}

// ConfigUpdate represents a configuration update
type ConfigUpdate struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	Key              string                 `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
	Value            string                 `protobuf:"bytes,2,opt,name=value,proto3" json:"value,omitempty"`
	ApplyImmediately bool                   `protobuf:"varint,3,opt,name=apply_immediately,json=applyImmediately,proto3" json:"apply_immediately,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ConfigUpdate) Reset() {
	*x = ConfigUpdate{}
	mi := &file_proto_inference_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ConfigUpdate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ConfigUpdate) ProtoMessage() {}

func (x *ConfigUpdate) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ConfigUpdate.ProtoReflect.Descriptor instead.
func (*ConfigUpdate) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{19}
}

func (x *ConfigUpdate) GetKey() string {
	if x != nil {
		return x.Key
	}
	return ""
}

func (x *ConfigUpdate) GetValue() string {
	if x != nil {
		return x.Value
	}
	return ""
}

func (x *ConfigUpdate) GetApplyImmediately() bool {
	if x != nil {
		return x.ApplyImmediately
	}
	return false
}

// ConfigUpdateResponse represents the response to a config update
type ConfigUpdateResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Success       bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error         string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	PreviousValue string                 `protobuf:"bytes,3,opt,name=previous_value,json=previousValue,proto3" json:"previous_value,omitempty"`
	NewValue      string                 `protobuf:"bytes,4,opt,name=new_value,json=newValue,proto3" json:"new_value,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ConfigUpdateResponse) Reset() {
	*x = ConfigUpdateResponse{}
	mi := &file_proto_inference_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ConfigUpdateResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ConfigUpdateResponse) ProtoMessage() {}

func (x *ConfigUpdateResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ConfigUpdateResponse.ProtoReflect.Descriptor instead.
func (*ConfigUpdateResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{20}
}

func (x *ConfigUpdateResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *ConfigUpdateResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *ConfigUpdateResponse) GetPreviousValue() string {
	if x != nil {
		return x.PreviousValue
	}
	return ""
}

func (x *ConfigUpdateResponse) GetNewValue() string {
	if x != nil {
		return x.NewValue
	}
	return ""
}

var File_proto_inference_proto protoreflect.FileDescriptor

const file_proto_inference_proto_rawDesc = "" +
	"\n" +
	"\x15proto/inference.proto\x12\tinference\"\xf1\x03\n" +
	"\x10InferenceRequest\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x1d\n" +
	"\n" +
	"model_name\x18\x02 \x01(\tR\tmodelName\x12\x16\n" +
	"\x06prompt\x18\x03 \x01(\tR\x06prompt\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\x04 \x01(\x05R\tmaxTokens\x12 \n" +
	"\vtemperature\x18\x05 \x01(\x02R\vtemperature\x12\x13\n" +
	"\x05top_p\x18\x06 \x01(\x02R\x04topP\x12\x13\n" +
	"\x05top_k\x18\a \x01(\x02R\x04topK\x12%\n" +
	"\x0estop_sequences\x18\b \x03(\tR\rstopSequences\x12\x16\n" +
	"\x06stream\x18\t \x01(\bR\x06stream\x12E\n" +
	"\bmetadata\x18\n" +
	" \x03(\v2).inference.InferenceRequest.MetadataEntryR\bmetadata\x12\x1d\n" +
	"\n" +
	"batch_size\x18\v \x01(\x05R\tbatchSize\x12\x1b\n" +
	"\tuse_cache\x18\f \x01(\bR\buseCache\x12\x1d\n" +
	"\n" +
	"timeout_ms\x18\r \x01(\x03R\ttimeoutMs\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xbe\x03\n" +
	"\x11InferenceResponse\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x1d\n" +
	"\n" +
	"model_name\x18\x02 \x01(\tR\tmodelName\x12\x16\n" +
	"\x06output\x18\x03 \x01(\tR\x06output\x12)\n" +
	"\x10tokens_generated\x18\x04 \x01(\x05R\x0ftokensGenerated\x12#\n" +
	"\rtokens_prompt\x18\x05 \x01(\x05R\ftokensPrompt\x12*\n" +
	"\x11inference_time_ms\x18\x06 \x01(\x03R\x0finferenceTimeMs\x12\"\n" +
	"\rqueue_time_ms\x18\a \x01(\x03R\vqueueTimeMs\x12F\n" +
	"\bmetadata\x18\b \x03(\v2*.inference.InferenceResponse.MetadataEntryR\bmetadata\x12\x18\n" +
	"\asuccess\x18\t \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\n" +
	" \x01(\tR\x05error\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xdc\x02\n" +
	"\x17InferenceStreamResponse\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x1f\n" +
	"\vchunk_index\x18\x02 \x01(\x05R\n" +
	"chunkIndex\x12\x14\n" +
	"\x05token\x18\x03 \x01(\tR\x05token\x12\x19\n" +
	"\bis_final\x18\x04 \x01(\bR\aisFinal\x12!\n" +
	"\ftotal_tokens\x18\x05 \x01(\x05R\vtotalTokens\x12\"\n" +
	"\rtotal_time_ms\x18\x06 \x01(\x03R\vtotalTimeMs\x12L\n" +
	"\bmetadata\x18\a \x03(\v20.inference.InferenceStreamResponse.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xbe\x03\n" +
	"\x10LoadModelRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12\x1d\n" +
	"\n" +
	"model_path\x18\x02 \x01(\tR\tmodelPath\x12\x16\n" +
	"\x06device\x18\x03 \x01(\tR\x06device\x12\"\n" +
	"\rgpu_memory_mb\x18\x04 \x01(\x05R\vgpuMemoryMb\x12\"\n" +
	"\fquantization\x18\x05 \x01(\tR\fquantization\x120\n" +
	"\x14tensor_parallel_size\x18\x06 \x01(\x05R\x12tensorParallelSize\x12\"\n" +
	"\rmax_model_len\x18\a \x01(\x05R\vmaxModelLen\x122\n" +
	"\x15enable_prefix_caching\x18\b \x01(\bR\x13enablePrefixCaching\x12E\n" +
	"\bmetadata\x18\t \x03(\v2).inference.LoadModelRequest.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xc5\x02\n" +
	"\x11LoadModelResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\x12\x1d\n" +
	"\n" +
	"model_name\x18\x03 \x01(\tR\tmodelName\x12\"\n" +
	"\rmodel_size_mb\x18\x04 \x01(\x03R\vmodelSizeMb\x12 \n" +
	"\fload_time_ms\x18\x05 \x01(\x03R\n" +
	"loadTimeMs\x12\x16\n" +
	"\x06device\x18\x06 \x01(\tR\x06device\x12F\n" +
	"\bmetadata\x18\a \x03(\v2*.inference.LoadModelResponse.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"I\n" +
	"\x12UnloadModelRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12\x14\n" +
	"\x05force\x18\x02 \x01(\bR\x05force\"\x8a\x01\n" +
	"\x13UnloadModelResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\x12\x1d\n" +
	"\n" +
	"model_name\x18\x03 \x01(\tR\tmodelName\x12$\n" +
	"\x0eunload_time_ms\x18\x04 \x01(\x03R\funloadTimeMs\"4\n" +
	"\x13GetModelInfoRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\"\xd6\x04\n" +
	"\x14GetModelInfoResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\x12\x1d\n" +
	"\n" +
	"model_name\x18\x03 \x01(\tR\tmodelName\x12\x1d\n" +
	"\n" +
	"model_path\x18\x04 \x01(\tR\tmodelPath\x12\x1b\n" +
	"\tis_loaded\x18\x05 \x01(\bR\bisLoaded\x12&\n" +
	"\x0fmemory_usage_mb\x18\x06 \x01(\x03R\rmemoryUsageMb\x12\x16\n" +
	"\x06device\x18\a \x01(\tR\x06device\x12,\n" +
	"\x12max_context_length\x18\b \x01(\x05R\x10maxContextLength\x12-\n" +
	"\x12supported_features\x18\t \x03(\tR\x11supportedFeatures\x12%\n" +
	"\x0etotal_requests\x18\n" +
	" \x01(\x03R\rtotalRequests\x124\n" +
	"\x16total_tokens_generated\x18\v \x01(\x03R\x14totalTokensGenerated\x121\n" +
	"\x15avg_inference_time_ms\x18\f \x01(\x02R\x12avgInferenceTimeMs\x12I\n" +
	"\bmetadata\x18\r \x03(\v2-.inference.GetModelInfoResponse.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"0\n" +
	"\x12HealthCheckRequest\x12\x1a\n" +
	"\bdetailed\x18\x01 \x01(\bR\bdetailed\"\x81\x05\n" +
	"\x13HealthCheckResponse\x12\x16\n" +
	"\x06status\x18\x01 \x01(\tR\x06status\x12*\n" +
	"\x11cpu_usage_percent\x18\x02 \x01(\x02R\x0fcpuUsagePercent\x120\n" +
	"\x14memory_usage_percent\x18\x03 \x01(\x02R\x12memoryUsagePercent\x12*\n" +
	"\x11gpu_usage_percent\x18\x04 \x01(\x02R\x0fgpuUsagePercent\x127\n" +
	"\x18gpu_memory_usage_percent\x18\x05 \x01(\x02R\x15gpuMemoryUsagePercent\x12'\n" +
	"\x0factive_requests\x18\x06 \x01(\x05R\x0eactiveRequests\x12'\n" +
	"\x0fqueued_requests\x18\a \x01(\x05R\x0equeuedRequests\x12#\n" +
	"\rloaded_models\x18\b \x01(\x05R\floadedModels\x12%\n" +
	"\x0euptime_seconds\x18\t \x01(\x03R\ruptimeSeconds\x12/\n" +
	"\x14last_request_time_ms\x18\n" +
	" \x01(\x03R\x11lastRequestTimeMs\x129\n" +
	"\fmodel_health\x18\v \x03(\v2\x16.inference.ModelHealthR\vmodelHealth\x12H\n" +
	"\bmetadata\x18\f \x03(\v2,.inference.HealthCheckResponse.MetadataEntryR\bmetadata\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xd8\x01\n" +
	"\vModelHealth\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12\x1d\n" +
	"\n" +
	"is_healthy\x18\x02 \x01(\bR\tisHealthy\x12&\n" +
	"\x0fmemory_usage_mb\x18\x03 \x01(\x03R\rmemoryUsageMb\x12'\n" +
	"\x0factive_requests\x18\x04 \x01(\x05R\x0eactiveRequests\x12$\n" +
	"\x0eavg_latency_ms\x18\x05 \x01(\x02R\favgLatencyMs\x12\x14\n" +
	"\x05error\x18\x06 \x01(\tR\x05error\"\x8b\x01\n" +
	"\x15BatchInferenceRequest\x127\n" +
	"\brequests\x18\x01 \x03(\v2\x1b.inference.InferenceRequestR\brequests\x12\x1a\n" +
	"\bparallel\x18\x02 \x01(\bR\bparallel\x12\x1d\n" +
	"\n" +
	"timeout_ms\x18\x03 \x01(\x03R\ttimeoutMs\"\xb0\x01\n" +
	"\x16BatchInferenceResponse\x12:\n" +
	"\tresponses\x18\x01 \x03(\v2\x1c.inference.InferenceResponseR\tresponses\x12\x1e\n" +
	"\n" +
	"successful\x18\x02 \x01(\x05R\n" +
	"successful\x12\x16\n" +
	"\x06failed\x18\x03 \x01(\x05R\x06failed\x12\"\n" +
	"\rtotal_time_ms\x18\x04 \x01(\x03R\vtotalTimeMs\"r\n" +
	"\x0fTokenizeRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12\x12\n" +
	"\x04text\x18\x02 \x01(\tR\x04text\x12,\n" +
	"\x12add_special_tokens\x18\x03 \x01(\bR\x10addSpecialTokens\"\x98\x01\n" +
	"\x10TokenizeResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\x12\x1b\n" +
	"\ttoken_ids\x18\x03 \x03(\x05R\btokenIds\x12\x16\n" +
	"\x06tokens\x18\x04 \x03(\tR\x06tokens\x12\x1f\n" +
	"\vtoken_count\x18\x05 \x01(\x05R\n" +
	"tokenCount\"\x7f\n" +
	"\x11DetokenizeRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12\x1b\n" +
	"\ttoken_ids\x18\x02 \x03(\x05R\btokenIds\x12.\n" +
	"\x13skip_special_tokens\x18\x03 \x01(\bR\x11skipSpecialTokens\"X\n" +
	"\x12DetokenizeResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\x12\x12\n" +
	"\x04text\x18\x03 \x01(\tR\x04text\"\x9d\x05\n" +
	"\vWorkerStats\x12\x1b\n" +
	"\tworker_id\x18\x01 \x01(\tR\bworkerId\x12\x16\n" +
	"\x06status\x18\x02 \x01(\tR\x06status\x12\x1b\n" +
	"\tcpu_usage\x18\x03 \x01(\x02R\bcpuUsage\x12&\n" +
	"\x0fmemory_usage_mb\x18\x04 \x01(\x02R\rmemoryUsageMb\x12\x1b\n" +
	"\tgpu_usage\x18\x05 \x01(\x02R\bgpuUsage\x12\"\n" +
	"\rgpu_memory_mb\x18\x06 \x01(\x02R\vgpuMemoryMb\x12%\n" +
	"\x0etotal_requests\x18\a \x01(\x03R\rtotalRequests\x12/\n" +
	"\x13successful_requests\x18\b \x01(\x03R\x12successfulRequests\x12'\n" +
	"\x0ffailed_requests\x18\t \x01(\x03R\x0efailedRequests\x121\n" +
	"\x15avg_inference_time_ms\x18\n" +
	" \x01(\x02R\x12avgInferenceTimeMs\x12$\n" +
	"\x0ep50_latency_ms\x18\v \x01(\x02R\fp50LatencyMs\x12$\n" +
	"\x0ep95_latency_ms\x18\f \x01(\x02R\fp95LatencyMs\x12$\n" +
	"\x0ep99_latency_ms\x18\r \x01(\x02R\fp99LatencyMs\x12'\n" +
	"\x0factive_requests\x18\x0e \x01(\x05R\x0eactiveRequests\x12'\n" +
	"\x0fqueued_requests\x18\x0f \x01(\x05R\x0equeuedRequests\x12%\n" +
	"\x0euptime_seconds\x18\x10 \x01(\x03R\ruptimeSeconds\x124\n" +
	"\x16last_request_timestamp\x18\x11 \x01(\x03R\x14lastRequestTimestamp\"c\n" +
	"\fConfigUpdate\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value\x12+\n" +
	"\x11apply_immediately\x18\x03 \x01(\bR\x10applyImmediately\"\x8a\x01\n" +
	"\x14ConfigUpdateResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\x12%\n" +
	"\x0eprevious_value\x18\x03 \x01(\tR\rpreviousValue\x12\x1b\n" +
	"\tnew_value\x18\x04 \x01(\tR\bnewValue2\xe5\x03\n" +
	"\x10InferenceService\x12F\n" +
	"\tInference\x12\x1b.inference.InferenceRequest\x1a\x1c.inference.InferenceResponse\x12T\n" +
	"\x0fStreamInference\x12\x1b.inference.InferenceRequest\x1a\".inference.InferenceStreamResponse0\x01\x12F\n" +
	"\tLoadModel\x12\x1b.inference.LoadModelRequest\x1a\x1c.inference.LoadModelResponse\x12L\n" +
	"\vUnloadModel\x12\x1d.inference.UnloadModelRequest\x1a\x1e.inference.UnloadModelResponse\x12O\n" +
	"\fGetModelInfo\x12\x1e.inference.GetModelInfoRequest\x1a\x1f.inference.GetModelInfoResponse\x12L\n" +
	"\vHealthCheck\x12\x1d.inference.HealthCheckRequest\x1a\x1e.inference.HealthCheckResponseB9Z7github.com/VighneshDev1411/velocityllm/internal/grpc/pbb\x06proto3"

var (
	file_proto_inference_proto_rawDescOnce sync.Once
	file_proto_inference_proto_rawDescData []byte
)

func file_proto_inference_proto_rawDescGZIP() []byte {
	file_proto_inference_proto_rawDescOnce.Do(func() {
		file_proto_inference_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_proto_inference_proto_rawDesc), len(file_proto_inference_proto_rawDesc)))
	})
	return file_proto_inference_proto_rawDescData
}

var file_proto_inference_proto_msgTypes = make([]protoimpl.MessageInfo, 28)
var file_proto_inference_proto_goTypes = []any{
	(*InferenceRequest)(nil),        // 0: inference.InferenceRequest
	(*InferenceResponse)(nil),       // 1: inference.InferenceResponse
	(*InferenceStreamResponse)(nil), // 2: inference.InferenceStreamResponse
	(*LoadModelRequest)(nil),        // 3: inference.LoadModelRequest
	(*LoadModelResponse)(nil),       // 4: inference.LoadModelResponse
	(*UnloadModelRequest)(nil),      // 5: inference.UnloadModelRequest
	(*UnloadModelResponse)(nil),     // 6: inference.UnloadModelResponse
	(*GetModelInfoRequest)(nil),     // 7: inference.GetModelInfoRequest
	(*GetModelInfoResponse)(nil),    // 8: inference.GetModelInfoResponse
	(*HealthCheckRequest)(nil),      // 9: inference.HealthCheckRequest
	(*HealthCheckResponse)(nil),     // 10: inference.HealthCheckResponse
	(*ModelHealth)(nil),             // 11: inference.ModelHealth
	(*BatchInferenceRequest)(nil),   // 12: inference.BatchInferenceRequest
	(*BatchInferenceResponse)(nil),  // 13: inference.BatchInferenceResponse
	(*TokenizeRequest)(nil),         // 14: inference.TokenizeRequest
	(*TokenizeResponse)(nil),        // 15: inference.TokenizeResponse
	(*DetokenizeRequest)(nil),       // 16: inference.DetokenizeRequest
	(*DetokenizeResponse)(nil),      // 17: inference.DetokenizeResponse
	(*WorkerStats)(nil),             // 18: inference.WorkerStats
	(*ConfigUpdate)(nil),            // 19: inference.ConfigUpdate
	(*ConfigUpdateResponse)(nil),    // 20: inference.ConfigUpdateResponse
	nil,                             // 21: inference.InferenceRequest.MetadataEntry
	nil,                             // 22: inference.InferenceResponse.MetadataEntry
	nil,                             // 23: inference.InferenceStreamResponse.MetadataEntry
	nil,                             // 24: inference.LoadModelRequest.MetadataEntry
	nil,                             // 25: inference.LoadModelResponse.MetadataEntry
	nil,                             // 26: inference.GetModelInfoResponse.MetadataEntry
	nil,                             // 27: inference.HealthCheckResponse.MetadataEntry
}
var file_proto_inference_proto_depIdxs = []int32{
	21, // 0: inference.InferenceRequest.metadata:type_name -> inference.InferenceRequest.MetadataEntry
	22, // 1: inference.InferenceResponse.metadata:type_name -> inference.InferenceResponse.MetadataEntry
	23, // 2: inference.InferenceStreamResponse.metadata:type_name -> inference.InferenceStreamResponse.MetadataEntry
	24, // 3: inference.LoadModelRequest.metadata:type_name -> inference.LoadModelRequest.MetadataEntry
	25, // 4: inference.LoadModelResponse.metadata:type_name -> inference.LoadModelResponse.MetadataEntry
	26, // 5: inference.GetModelInfoResponse.metadata:type_name -> inference.GetModelInfoResponse.MetadataEntry
	11, // 6: inference.HealthCheckResponse.model_health:type_name -> inference.ModelHealth
	27, // 7: inference.HealthCheckResponse.metadata:type_name -> inference.HealthCheckResponse.MetadataEntry
	0,  // 8: inference.BatchInferenceRequest.requests:type_name -> inference.InferenceRequest
	1,  // 9: inference.BatchInferenceResponse.responses:type_name -> inference.InferenceResponse
	0,  // 10: inference.InferenceService.Inference:input_type -> inference.InferenceRequest
	0,  // 11: inference.InferenceService.StreamInference:input_type -> inference.InferenceRequest
	3,  // 12: inference.InferenceService.LoadModel:input_type -> inference.LoadModelRequest
	5,  // 13: inference.InferenceService.UnloadModel:input_type -> inference.UnloadModelRequest
	7,  // 14: inference.InferenceService.GetModelInfo:input_type -> inference.GetModelInfoRequest
	9,  // 15: inference.InferenceService.HealthCheck:input_type -> inference.HealthCheckRequest
	1,  // 16: inference.InferenceService.Inference:output_type -> inference.InferenceResponse
	2,  // 17: inference.InferenceService.StreamInference:output_type -> inference.InferenceStreamResponse
	4,  // 18: inference.InferenceService.LoadModel:output_type -> inference.LoadModelResponse
	6,  // 19: inference.InferenceService.UnloadModel:output_type -> inference.UnloadModelResponse
	8,  // 20: inference.InferenceService.GetModelInfo:output_type -> inference.GetModelInfoResponse
	10, // 21: inference.InferenceService.HealthCheck:output_type -> inference.HealthCheckResponse
	16, // [16:22] is the sub-list for method output_type
	10, // [10:16] is the sub-list for method input_type
	10, // [10:10] is the sub-list for extension type_name
	10, // [10:10] is the sub-list for extension extendee
	0,  // [0:10] is the sub-list for field type_name
}

func init() { file_proto_inference_proto_init() }
func file_proto_inference_proto_init() {
	if File_proto_inference_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_proto_inference_proto_rawDesc), len(file_proto_inference_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   28,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_proto_inference_proto_goTypes,
		DependencyIndexes: file_proto_inference_proto_depIdxs,
		MessageInfos:      file_proto_inference_proto_msgTypes,
	}.Build()
	File_proto_inference_proto = out.File
	file_proto_inference_proto_goTypes = nil
	file_proto_inference_proto_depIdxs = nil
}
